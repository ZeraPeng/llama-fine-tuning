{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1xgCSsyrjilX29EYCW0ZXKpyCk_bD12og",
      "authorship_tag": "ABX9TyPDKdctFUqwEtGy1O1cZKu7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZeraPeng/llama-fine-tuning/blob/main/Llama_Efficient_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jXN4Rw0jzDpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c200df-8da2-4bcb-d231-351ca9402a0f",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Efficient-Tuning'...\n",
            "remote: Enumerating objects: 3279, done.\u001b[K\n",
            "remote: Counting objects: 100% (891/891), done.\u001b[K\n",
            "remote: Compressing objects: 100% (225/225), done.\u001b[K\n",
            "remote: Total 3279 (delta 733), reused 732 (delta 664), pack-reused 2388\u001b[K\n",
            "Receiving objects: 100% (3279/3279), 179.13 MiB | 28.41 MiB/s, done.\n",
            "Resolving deltas: 100% (2324/2324), done.\n",
            "Updating files: 100% (119/119), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/hiyouga/LLaMA-Efficient-Tuning.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LLaMA-Efficient-Tuning/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGM3dUWIUF5O",
        "outputId": "63bde2e2-0868-417f-8e2e-c05c93586fce",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Efficient-Tuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU3FLA6bUQnv",
        "outputId": "4b389c73-3f48-4518-a9bf-fc5d9792ddeb",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.0.1+cu118)\n",
            "Collecting transformers>=4.30.0 (from -r requirements.txt (line 2))\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.12.0 (from -r requirements.txt (line 3))\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.21.0 (from -r requirements.txt (line 4))\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft>=0.4.0 (from -r requirements.txt (line 5))\n",
            "  Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.6/85.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl>=0.7.1 (from -r requirements.txt (line 6))\n",
            "  Downloading trl-0.7.1-py3-none-any.whl (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.0/118.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.11.3)\n",
            "Collecting sentencepiece (from -r requirements.txt (line 8))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (3.20.3)\n",
            "Collecting tiktoken (from -r requirements.txt (line 10))\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.42.1)\n",
            "Collecting rouge-chinese (from -r requirements.txt (line 12))\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (3.8.1)\n",
            "Collecting gradio>=3.36.0 (from -r requirements.txt (line 14))\n",
            "  Downloading gradio-3.46.0-py3-none-any.whl (20.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn (from -r requirements.txt (line 15))\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic==1.10.11 (from -r requirements.txt (line 16))\n",
            "  Downloading pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi==0.95.1 (from -r requirements.txt (line 17))\n",
            "  Downloading fastapi-0.95.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sse-starlette (from -r requirements.txt (line 18))\n",
            "  Downloading sse_starlette-1.6.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 19)) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.10.11->-r requirements.txt (line 16)) (4.5.0)\n",
            "Collecting starlette<0.27.0,>=0.26.1 (from fastapi==0.95.1->-r requirements.txt (line 17))\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.12.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.1->-r requirements.txt (line 1)) (3.27.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.1->-r requirements.txt (line 1)) (17.0.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers>=4.30.0->-r requirements.txt (line 2))\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers>=4.30.0->-r requirements.txt (line 2))\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.30.0->-r requirements.txt (line 2))\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 3)) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.12.0->-r requirements.txt (line 3))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 3)) (1.5.3)\n",
            "Collecting xxhash (from datasets>=2.12.0->-r requirements.txt (line 3))\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.12.0->-r requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 3)) (3.8.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge-chinese->-r requirements.txt (line 12)) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 13)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 13)) (1.3.2)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=3.36.0->-r requirements.txt (line 14))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.36.0->-r requirements.txt (line 14)) (4.2.2)\n",
            "Collecting ffmpy (from gradio>=3.36.0->-r requirements.txt (line 14))\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.5.3 (from gradio>=3.36.0->-r requirements.txt (line 14))\n",
            "  Downloading gradio_client-0.5.3-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.4/298.4 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio>=3.36.0->-r requirements.txt (line 14))\n",
            "  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.36.0->-r requirements.txt (line 14)) (6.1.0)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.36.0->-r requirements.txt (line 14)) (2.1.3)\n",
            "Collecting orjson~=3.0 (from gradio>=3.36.0->-r requirements.txt (line 14))\n",
            "  Downloading orjson-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.36.0->-r requirements.txt (line 14)) (9.4.0)\n",
            "Collecting pydub (from gradio>=3.36.0->-r requirements.txt (line 14))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio>=3.36.0->-r requirements.txt (line 14))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio>=3.36.0->-r requirements.txt (line 14))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio>=3.36.0->-r requirements.txt (line 14))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11>=0.8 (from uvicorn->-r requirements.txt (line 15))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (4.43.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 19)) (2.8.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=3.36.0->-r requirements.txt (line 14)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=3.36.0->-r requirements.txt (line 14)) (4.19.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio>=3.36.0->-r requirements.txt (line 14)) (0.12.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.12.0->-r requirements.txt (line 3)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.12.0->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.12.0->-r requirements.txt (line 3)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.12.0->-r requirements.txt (line 3)) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.12.0->-r requirements.txt (line 3)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.12.0->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.12.0->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.12.0->-r requirements.txt (line 3)) (2023.3.post1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 2)) (2023.7.22)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.27.0,>=0.26.1->fastapi==0.95.1->-r requirements.txt (line 17)) (3.7.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers>=4.30.0->-r requirements.txt (line 2))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.19.0,>=0.18.0 (from httpx->gradio>=3.36.0->-r requirements.txt (line 14))\n",
            "  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio>=3.36.0->-r requirements.txt (line 14)) (1.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.27.0,>=0.26.1->fastapi==0.95.1->-r requirements.txt (line 17)) (1.1.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=3.36.0->-r requirements.txt (line 14)) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=3.36.0->-r requirements.txt (line 14)) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=3.36.0->-r requirements.txt (line 14)) (0.10.3)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=f875d85d967cc824443fbb7e6b5b5d9b3d2a6507848db2a7fadadc0db496ef2e\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: sentencepiece, safetensors, pydub, ffmpy, xxhash, websockets, semantic-version, rouge-chinese, python-multipart, pydantic, orjson, h11, dill, aiofiles, uvicorn, tiktoken, starlette, multiprocess, huggingface-hub, httpcore, tokenizers, sse-starlette, httpx, fastapi, transformers, gradio-client, datasets, gradio, accelerate, trl, peft\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "Successfully installed accelerate-0.23.0 aiofiles-23.2.1 datasets-2.14.5 dill-0.3.7 fastapi-0.95.1 ffmpy-0.3.1 gradio-3.46.0 gradio-client-0.5.3 h11-0.14.0 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.16.4 multiprocess-0.70.15 orjson-3.9.7 peft-0.5.0 pydantic-1.10.11 pydub-0.25.1 python-multipart-0.0.6 rouge-chinese-1.0.3 safetensors-0.3.3 semantic-version-2.10.0 sentencepiece-0.1.99 sse-starlette-1.6.5 starlette-0.26.1 tiktoken-0.5.1 tokenizers-0.14.0 transformers-4.34.0 trl-0.7.1 uvicorn-0.23.2 websockets-11.0.3 xxhash-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path dary1149/llama-2-7b-chat-hf \\\n",
        "    --do_train \\\n",
        "    --dataset alpaca_gpt4_zh \\\n",
        "    --template default \\\n",
        "    --finetuning_type lora \\\n",
        "    --lora_target q_proj,v_proj \\\n",
        "    --output_dir alpaca_gpt4_zh_ckpoint \\\n",
        "    --overwrite_cache \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --logging_steps 10 \\\n",
        "    --save_steps 1000 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --plot_loss \\\n",
        "    --fp16"
      ],
      "metadata": {
        "id": "pLqtcjyuyYcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/ywchiu/largitdata/master/data/largitdata_llm_sample.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWMGZIK5xcfv",
        "outputId": "9b2fc9b0-d767-493e-add3-c4d77e859a0f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-04 10:36:43--  https://raw.githubusercontent.com/ywchiu/largitdata/master/data/largitdata_llm_sample.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 708651 (692K) [text/plain]\n",
            "Saving to: ‘largitdata_llm_sample.json’\n",
            "\n",
            "largitdata_llm_samp 100%[===================>] 692.04K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-10-04 10:36:44 (13.2 MB/s) - ‘largitdata_llm_sample.json’ saved [708651/708651]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp largitdata_llm_sample.json data/"
      ],
      "metadata": {
        "id": "9_brIocv6wrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "173586d2-a6d5-4dbd-edae-0b5e6de41783"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'largitdata_llm_sample.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oowtEYW68o45",
        "outputId": "458f3312-5f34-4063-9936-493f57c265a1",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ROx9MD1e7nhu",
        "outputId": "af75fe6c-46b2-4697-9434-87c413393be7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! CUDA_VISIBLE_DEVICES=0 python src/train_bash.py \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path meta-llama/Llama-2-7b-chat-hf \\\n",
        "    --do_train \\\n",
        "    --dataset largitdata \\\n",
        "    --template default \\\n",
        "    --finetuning_type lora \\\n",
        "    --lora_target q_proj,v_proj \\\n",
        "    --output_dir largitdata_ckpoint \\\n",
        "    --overwrite_output_dir yes \\\n",
        "    --overwrite_cache \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --logging_steps 10 \\\n",
        "    --save_steps 10 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --plot_loss \\\n",
        "    --fp16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc6OleC0U6eh",
        "outputId": "4ec3b4ef-811c-4523-fe3b-de0faa134b33"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-10-03 03:31:06.548675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "FlashAttention-2 is not installed, ignore this if you are not using FlashAttention.\n",
            "10/03/2023 03:31:09 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
            "[INFO|training_args.py:1332] 2023-10-03 03:31:09,783 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1764] 2023-10-03 03:31:09,783 >> PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1677: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "10/03/2023 03:31:09 - INFO - llmtuner.tuner.core.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
            "  distributed training: True, compute dtype: torch.float16\n",
            "10/03/2023 03:31:09 - INFO - llmtuner.tuner.core.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=4,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=largitdata_ckpoint/runs/Oct03_03-31-09_ce5c35a1ac9c,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=largitdata_ckpoint,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=largitdata_ckpoint,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=10,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "10/03/2023 03:31:09 - INFO - llmtuner.dsets.loader - Loading dataset largitdata_llm_sample.json...\n",
            "10/03/2023 03:31:09 - WARNING - llmtuner.dsets.utils - Checksum failed: mismatched SHA-1 hash value at data/largitdata_llm_sample.json.\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
            "You can remove this warning by passing 'token=None' instead.\n",
            "  warnings.warn(\n",
            "Using custom data configuration default-07d96443c3ebc601\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-07d96443c3ebc601/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-07d96443c3ebc601/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 10485.76it/s]\n",
            "Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1562.71it/s]\n",
            "Generating train split\n",
            "Generating train split: 1039 examples [00:00, 32942.38 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-07d96443c3ebc601/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n",
            "Downloading (…)okenizer_config.json: 100% 776/776 [00:00<00:00, 4.70MB/s]\n",
            "Downloading tokenizer.model: 100% 500k/500k [00:00<00:00, 36.2MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.84M/1.84M [00:01<00:00, 1.83MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 414/414 [00:00<00:00, 2.45MB/s]\n",
            "[INFO|tokenization_utils_base.py:1852] 2023-10-03 03:31:15,805 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/08751db2aca9bf2f7f80d2e516117a53d7450235/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:1852] 2023-10-03 03:31:15,805 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/08751db2aca9bf2f7f80d2e516117a53d7450235/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1852] 2023-10-03 03:31:15,805 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1852] 2023-10-03 03:31:15,805 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/08751db2aca9bf2f7f80d2e516117a53d7450235/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1852] 2023-10-03 03:31:15,805 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/08751db2aca9bf2f7f80d2e516117a53d7450235/tokenizer_config.json\n",
            "Downloading (…)lve/main/config.json: 100% 614/614 [00:00<00:00, 3.26MB/s]\n",
            "[INFO|configuration_utils.py:715] 2023-10-03 03:31:16,698 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/08751db2aca9bf2f7f80d2e516117a53d7450235/config.json\n",
            "[INFO|configuration_utils.py:775] 2023-10-03 03:31:16,699 >> Model config LlamaConfig {\n",
            "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.33.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "Downloading (…)fetensors.index.json: 100% 26.8k/26.8k [00:00<00:00, 84.0MB/s]\n",
            "[INFO|modeling_utils.py:2862] 2023-10-03 03:31:18,401 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/08751db2aca9bf2f7f80d2e516117a53d7450235/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "Downloading (…)of-00002.safetensors:   0% 0.00/9.98G [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   0% 31.5M/9.98G [00:00<00:33, 295MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   1% 94.4M/9.98G [00:00<00:20, 472MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   1% 147M/9.98G [00:00<00:20, 482MB/s] \u001b[A\n",
            "Downloading (…)of-00002.safetensors:   2% 199M/9.98G [00:00<00:19, 491MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   3% 252M/9.98G [00:00<00:19, 491MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   3% 304M/9.98G [00:00<00:20, 471MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   4% 357M/9.98G [00:00<00:21, 452MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   4% 409M/9.98G [00:00<00:21, 451MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   5% 461M/9.98G [00:01<00:20, 459MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   5% 514M/9.98G [00:01<00:21, 433MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   6% 566M/9.98G [00:01<00:23, 408MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   6% 619M/9.98G [00:01<00:22, 425MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   7% 671M/9.98G [00:01<00:22, 420MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   7% 724M/9.98G [00:01<00:22, 420MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   8% 786M/9.98G [00:01<00:19, 461MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   9% 849M/9.98G [00:01<00:18, 499MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   9% 902M/9.98G [00:01<00:18, 478MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  10% 954M/9.98G [00:02<00:21, 419MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  10% 1.01G/9.98G [00:02<00:23, 387MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  11% 1.06G/9.98G [00:02<00:21, 413MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  11% 1.11G/9.98G [00:02<00:20, 425MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  12% 1.16G/9.98G [00:02<00:21, 419MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  12% 1.22G/9.98G [00:02<00:20, 419MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  13% 1.27G/9.98G [00:02<00:21, 413MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  13% 1.31G/9.98G [00:03<00:21, 407MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  14% 1.36G/9.98G [00:03<00:20, 421MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  14% 1.42G/9.98G [00:03<00:19, 440MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  15% 1.47G/9.98G [00:03<00:18, 453MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  15% 1.52G/9.98G [00:03<00:19, 429MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  16% 1.57G/9.98G [00:03<00:21, 392MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  16% 1.61G/9.98G [00:03<00:21, 394MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  17% 1.67G/9.98G [00:03<00:19, 424MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  17% 1.72G/9.98G [00:03<00:18, 443MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  18% 1.77G/9.98G [00:04<00:17, 460MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  18% 1.82G/9.98G [00:04<00:17, 473MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  19% 1.88G/9.98G [00:04<00:16, 484MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  19% 1.93G/9.98G [00:04<00:16, 492MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  20% 1.98G/9.98G [00:04<00:16, 498MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  20% 2.03G/9.98G [00:07<02:32, 51.9MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  21% 2.09G/9.98G [00:07<01:51, 70.9MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  21% 2.14G/9.98G [00:07<01:23, 94.2MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  22% 2.18G/9.98G [00:07<01:06, 117MB/s] \u001b[A\n",
            "Downloading (…)of-00002.safetensors:  22% 2.23G/9.98G [00:08<00:50, 153MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  23% 2.29G/9.98G [00:08<00:39, 192MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  23% 2.34G/9.98G [00:08<00:33, 229MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  24% 2.38G/9.98G [00:08<00:29, 259MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  24% 2.43G/9.98G [00:08<00:25, 298MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  25% 2.49G/9.98G [00:08<00:22, 327MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  25% 2.54G/9.98G [00:08<00:20, 361MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  26% 2.59G/9.98G [00:08<00:19, 386MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  26% 2.64G/9.98G [00:08<00:18, 387MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  27% 2.69G/9.98G [00:09<00:23, 314MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  28% 2.75G/9.98G [00:09<00:20, 345MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  28% 2.80G/9.98G [00:09<00:19, 366MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  28% 2.84G/9.98G [00:09<00:19, 368MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  29% 2.89G/9.98G [00:09<00:17, 400MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  30% 2.95G/9.98G [00:09<00:16, 427MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  30% 3.01G/9.98G [00:09<00:15, 458MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  31% 3.06G/9.98G [00:10<00:15, 455MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  31% 3.11G/9.98G [00:10<00:14, 468MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  32% 3.17G/9.98G [00:10<00:14, 482MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  32% 3.22G/9.98G [00:10<00:14, 461MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  33% 3.27G/9.98G [00:10<00:15, 437MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  33% 3.32G/9.98G [00:10<00:15, 434MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  34% 3.39G/9.98G [00:10<00:14, 455MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  34% 3.44G/9.98G [00:10<00:14, 448MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  35% 3.49G/9.98G [00:11<00:17, 364MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  35% 3.53G/9.98G [00:11<00:19, 333MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  36% 3.58G/9.98G [00:11<00:19, 321MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  36% 3.62G/9.98G [00:11<00:27, 229MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  37% 3.66G/9.98G [00:11<00:24, 262MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  37% 3.70G/9.98G [00:11<00:22, 283MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  38% 3.74G/9.98G [00:12<00:21, 294MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  38% 3.79G/9.98G [00:12<00:19, 311MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  38% 3.83G/9.98G [00:12<00:19, 317MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  39% 3.87G/9.98G [00:12<00:18, 324MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  39% 3.91G/9.98G [00:12<00:18, 327MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  40% 3.95G/9.98G [00:12<00:18, 330MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  40% 4.00G/9.98G [00:12<00:17, 337MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  40% 4.04G/9.98G [00:12<00:17, 339MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  41% 4.08G/9.98G [00:13<00:17, 343MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  41% 4.12G/9.98G [00:13<00:18, 312MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  42% 4.16G/9.98G [00:13<00:17, 326MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  42% 4.22G/9.98G [00:13<00:16, 358MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  43% 4.26G/9.98G [00:13<00:15, 368MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  43% 4.30G/9.98G [00:13<00:15, 373MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  44% 4.35G/9.98G [00:13<00:13, 404MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  44% 4.40G/9.98G [00:13<00:12, 433MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  45% 4.46G/9.98G [00:13<00:13, 420MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  45% 4.51G/9.98G [00:14<00:13, 414MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  46% 4.56G/9.98G [00:14<00:12, 419MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  46% 4.61G/9.98G [00:14<00:13, 410MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  47% 4.66G/9.98G [00:14<00:13, 383MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  47% 4.71G/9.98G [00:14<00:12, 415MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  48% 4.76G/9.98G [00:14<00:12, 426MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  48% 4.81G/9.98G [00:14<00:12, 429MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  49% 4.87G/9.98G [00:14<00:11, 428MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  49% 4.92G/9.98G [00:15<00:12, 415MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  50% 4.97G/9.98G [00:15<00:12, 414MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  50% 5.03G/9.98G [00:15<00:11, 448MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  51% 5.09G/9.98G [00:15<00:10, 465MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  52% 5.14G/9.98G [00:15<00:10, 478MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  52% 5.19G/9.98G [00:15<00:11, 419MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  53% 5.24G/9.98G [00:15<00:13, 345MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  53% 5.28G/9.98G [00:16<00:15, 310MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  53% 5.33G/9.98G [00:16<00:16, 289MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  54% 5.36G/9.98G [00:16<00:16, 279MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  54% 5.39G/9.98G [00:16<00:16, 270MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  54% 5.42G/9.98G [00:16<00:17, 263MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  55% 5.45G/9.98G [00:16<00:17, 263MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  55% 5.48G/9.98G [00:16<00:17, 255MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  55% 5.52G/9.98G [00:17<00:17, 251MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  56% 5.55G/9.98G [00:17<00:17, 254MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  56% 5.58G/9.98G [00:17<00:17, 250MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  56% 5.61G/9.98G [00:17<00:17, 255MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  57% 5.64G/9.98G [00:17<00:16, 259MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  57% 5.67G/9.98G [00:17<00:16, 262MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  57% 5.70G/9.98G [00:17<00:15, 269MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  57% 5.74G/9.98G [00:17<00:16, 264MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  58% 5.77G/9.98G [00:17<00:15, 276MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  58% 5.80G/9.98G [00:18<00:14, 282MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  59% 5.84G/9.98G [00:18<00:12, 320MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  59% 5.88G/9.98G [00:18<00:13, 312MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  59% 5.92G/9.98G [00:18<00:12, 318MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  60% 5.97G/9.98G [00:18<00:12, 323MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  60% 6.01G/9.98G [00:18<00:12, 330MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  61% 6.05G/9.98G [00:18<00:11, 336MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  61% 6.09G/9.98G [00:18<00:11, 344MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  61% 6.13G/9.98G [00:19<00:10, 361MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  62% 6.19G/9.98G [00:19<00:09, 392MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  62% 6.23G/9.98G [00:19<00:12, 309MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  63% 6.28G/9.98G [00:19<00:10, 339MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  63% 6.33G/9.98G [00:19<00:09, 376MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  64% 6.39G/9.98G [00:19<00:09, 396MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  65% 6.44G/9.98G [00:19<00:08, 418MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  65% 6.49G/9.98G [00:19<00:08, 414MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  66% 6.54G/9.98G [00:20<00:08, 423MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  66% 6.60G/9.98G [00:20<00:07, 428MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  67% 6.65G/9.98G [00:20<00:07, 432MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  67% 6.71G/9.98G [00:20<00:07, 457MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  68% 6.76G/9.98G [00:20<00:07, 458MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  68% 6.83G/9.98G [00:20<00:06, 476MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  69% 6.88G/9.98G [00:20<00:06, 486MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  70% 6.94G/9.98G [00:20<00:05, 516MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  70% 6.99G/9.98G [00:20<00:05, 504MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  71% 7.05G/9.98G [00:21<00:06, 476MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  71% 7.10G/9.98G [00:21<00:07, 390MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  72% 7.14G/9.98G [00:21<00:08, 327MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  72% 7.18G/9.98G [00:21<00:09, 288MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  72% 7.22G/9.98G [00:21<00:10, 259MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  73% 7.26G/9.98G [00:22<00:11, 244MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  73% 7.29G/9.98G [00:22<00:11, 234MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  73% 7.32G/9.98G [00:22<00:11, 234MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  74% 7.35G/9.98G [00:22<00:11, 231MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  74% 7.38G/9.98G [00:22<00:11, 229MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  74% 7.41G/9.98G [00:22<00:10, 237MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  75% 7.44G/9.98G [00:22<00:10, 239MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  75% 7.48G/9.98G [00:22<00:10, 250MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  75% 7.52G/9.98G [00:23<00:08, 279MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  76% 7.56G/9.98G [00:23<00:08, 291MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  76% 7.60G/9.98G [00:23<00:07, 302MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  77% 7.63G/9.98G [00:23<00:07, 300MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  77% 7.68G/9.98G [00:23<00:06, 331MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  77% 7.72G/9.98G [00:23<00:06, 334MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  78% 7.77G/9.98G [00:23<00:06, 365MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  78% 7.82G/9.98G [00:23<00:05, 392MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  79% 7.87G/9.98G [00:24<00:05, 417MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  80% 7.94G/9.98G [00:24<00:04, 458MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  80% 7.99G/9.98G [00:24<00:04, 460MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  81% 8.04G/9.98G [00:24<00:04, 464MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  81% 8.10G/9.98G [00:24<00:04, 417MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  82% 8.15G/9.98G [00:24<00:04, 408MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  82% 8.20G/9.98G [00:24<00:04, 429MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  83% 8.25G/9.98G [00:24<00:03, 449MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  83% 8.30G/9.98G [00:24<00:03, 465MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  84% 8.36G/9.98G [00:25<00:03, 460MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  84% 8.41G/9.98G [00:25<00:03, 443MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  85% 8.46G/9.98G [00:25<00:03, 443MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  85% 8.51G/9.98G [00:25<00:03, 442MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  86% 8.57G/9.98G [00:25<00:03, 425MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  86% 8.62G/9.98G [00:25<00:03, 423MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  87% 8.67G/9.98G [00:25<00:02, 442MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  87% 8.72G/9.98G [00:25<00:03, 396MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  88% 8.78G/9.98G [00:26<00:02, 403MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  88% 8.83G/9.98G [00:26<00:02, 427MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  89% 8.88G/9.98G [00:26<00:02, 435MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  90% 8.93G/9.98G [00:26<00:02, 437MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  90% 8.99G/9.98G [00:26<00:02, 453MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  91% 9.04G/9.98G [00:26<00:02, 438MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  91% 9.09G/9.98G [00:26<00:02, 377MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  92% 9.13G/9.98G [00:27<00:02, 324MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  92% 9.18G/9.98G [00:27<00:02, 275MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  92% 9.21G/9.98G [00:27<00:04, 182MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  93% 9.25G/9.98G [00:27<00:03, 218MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  93% 9.28G/9.98G [00:27<00:03, 225MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  93% 9.31G/9.98G [00:28<00:03, 205MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  94% 9.34G/9.98G [00:28<00:03, 193MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  94% 9.37G/9.98G [00:28<00:03, 191MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  94% 9.41G/9.98G [00:28<00:03, 186MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  95% 9.44G/9.98G [00:28<00:02, 201MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  95% 9.47G/9.98G [00:28<00:02, 220MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  95% 9.50G/9.98G [00:28<00:02, 235MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  96% 9.53G/9.98G [00:29<00:01, 251MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  96% 9.57G/9.98G [00:29<00:01, 277MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  96% 9.63G/9.98G [00:29<00:01, 329MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  97% 9.67G/9.98G [00:29<00:01, 266MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  97% 9.71G/9.98G [00:29<00:00, 291MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  98% 9.76G/9.98G [00:29<00:00, 345MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  98% 9.83G/9.98G [00:29<00:00, 408MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  99% 9.88G/9.98G [00:29<00:00, 428MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors: 100% 9.98G/9.98G [00:30<00:00, 331MB/s]\n",
            "Downloading shards:  50% 1/2 [00:30<00:30, 30.62s/it]\n",
            "Downloading (…)of-00002.safetensors:   0% 0.00/3.50G [00:00<?, ?B/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   1% 41.9M/3.50G [00:00<00:08, 393MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   3% 94.4M/3.50G [00:00<00:07, 430MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   4% 147M/3.50G [00:00<00:07, 466MB/s] \u001b[A\n",
            "Downloading (…)of-00002.safetensors:   6% 199M/3.50G [00:00<00:06, 472MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   7% 252M/3.50G [00:00<00:07, 431MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:   9% 315M/3.50G [00:00<00:06, 476MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  10% 367M/3.50G [00:00<00:06, 457MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  12% 419M/3.50G [00:00<00:07, 437MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  14% 482M/3.50G [00:01<00:06, 475MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  15% 535M/3.50G [00:01<00:06, 481MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  17% 587M/3.50G [00:01<00:09, 310MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  18% 629M/3.50G [00:01<00:10, 272MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  19% 671M/3.50G [00:01<00:11, 239MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  20% 703M/3.50G [00:02<00:12, 224MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  21% 734M/3.50G [00:02<00:13, 209MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  22% 765M/3.50G [00:02<00:14, 195MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  22% 786M/3.50G [00:02<00:14, 182MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  23% 807M/3.50G [00:02<00:15, 175MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  24% 828M/3.50G [00:02<00:15, 169MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  24% 849M/3.50G [00:03<00:16, 165MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  25% 870M/3.50G [00:03<00:16, 164MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  25% 891M/3.50G [00:03<00:15, 165MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  26% 923M/3.50G [00:03<00:12, 200MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  28% 965M/3.50G [00:03<00:10, 238MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  28% 996M/3.50G [00:03<00:10, 245MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  30% 1.05G/3.50G [00:03<00:07, 307MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  31% 1.09G/3.50G [00:03<00:07, 325MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  33% 1.14G/3.50G [00:03<00:06, 360MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  34% 1.20G/3.50G [00:04<00:05, 394MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  36% 1.25G/3.50G [00:04<00:05, 409MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  37% 1.29G/3.50G [00:04<00:05, 410MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  38% 1.33G/3.50G [00:04<00:05, 404MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  39% 1.37G/3.50G [00:04<00:05, 380MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  40% 1.42G/3.50G [00:04<00:05, 367MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  42% 1.46G/3.50G [00:04<00:05, 356MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  43% 1.50G/3.50G [00:04<00:05, 349MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  44% 1.54G/3.50G [00:05<00:05, 334MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  45% 1.58G/3.50G [00:05<00:05, 323MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  46% 1.63G/3.50G [00:05<00:06, 309MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  47% 1.66G/3.50G [00:05<00:06, 303MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  48% 1.69G/3.50G [00:05<00:06, 299MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  49% 1.72G/3.50G [00:05<00:06, 292MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  50% 1.75G/3.50G [00:05<00:06, 283MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  51% 1.78G/3.50G [00:05<00:05, 290MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  52% 1.82G/3.50G [00:06<00:05, 311MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  53% 1.87G/3.50G [00:06<00:05, 317MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  55% 1.91G/3.50G [00:06<00:05, 313MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  56% 1.95G/3.50G [00:06<00:05, 307MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  57% 1.99G/3.50G [00:06<00:04, 309MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  58% 2.02G/3.50G [00:06<00:04, 310MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  59% 2.06G/3.50G [00:06<00:04, 304MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  60% 2.10G/3.50G [00:06<00:04, 319MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  61% 2.15G/3.50G [00:06<00:03, 356MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  63% 2.20G/3.50G [00:07<00:03, 382MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  64% 2.25G/3.50G [00:07<00:03, 405MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  66% 2.31G/3.50G [00:07<00:02, 418MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  67% 2.36G/3.50G [00:07<00:02, 429MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  69% 2.41G/3.50G [00:07<00:02, 439MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  70% 2.46G/3.50G [00:07<00:02, 447MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  72% 2.52G/3.50G [00:07<00:02, 453MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  73% 2.57G/3.50G [00:07<00:02, 458MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  75% 2.62G/3.50G [00:08<00:01, 462MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  76% 2.67G/3.50G [00:08<00:01, 463MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  78% 2.73G/3.50G [00:08<00:01, 466MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  79% 2.78G/3.50G [00:08<00:01, 467MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  81% 2.83G/3.50G [00:08<00:01, 467MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  82% 2.88G/3.50G [00:08<00:01, 466MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  84% 2.94G/3.50G [00:08<00:01, 451MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  85% 2.99G/3.50G [00:08<00:01, 449MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  87% 3.04G/3.50G [00:08<00:01, 455MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  88% 3.09G/3.50G [00:09<00:01, 339MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  90% 3.15G/3.50G [00:09<00:00, 371MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  91% 3.19G/3.50G [00:09<00:00, 382MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  93% 3.24G/3.50G [00:09<00:00, 405MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  94% 3.29G/3.50G [00:09<00:00, 373MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  95% 3.33G/3.50G [00:09<00:00, 353MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  96% 3.38G/3.50G [00:09<00:00, 319MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  98% 3.42G/3.50G [00:10<00:00, 311MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors:  99% 3.46G/3.50G [00:10<00:00, 294MB/s]\u001b[A\n",
            "Downloading (…)of-00002.safetensors: 100% 3.50G/3.50G [00:10<00:00, 335MB/s]\n",
            "Downloading shards: 100% 2/2 [00:41<00:00, 20.75s/it]\n",
            "[INFO|modeling_utils.py:1200] 2023-10-03 03:31:59,897 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:768] 2023-10-03 03:31:59,898 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"transformers_version\": \"4.33.3\"\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:00<00:00,  3.94it/s]\n",
            "[INFO|modeling_utils.py:3648] 2023-10-03 03:32:00,766 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:3656] 2023-10-03 03:32:00,766 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "Downloading (…)neration_config.json: 100% 188/188 [00:00<00:00, 901kB/s]\n",
            "[INFO|configuration_utils.py:730] 2023-10-03 03:32:01,587 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/08751db2aca9bf2f7f80d2e516117a53d7450235/generation_config.json\n",
            "[INFO|configuration_utils.py:768] 2023-10-03 03:32:01,588 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 4096,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"temperature\": 0.6,\n",
            "  \"top_p\": 0.9,\n",
            "  \"transformers_version\": \"4.33.3\"\n",
            "}\n",
            "\n",
            "10/03/2023 03:32:01 - INFO - llmtuner.tuner.core.adapter - Fine-tuning method: LoRA\n",
            "10/03/2023 03:32:16 - INFO - llmtuner.tuner.core.loader - trainable params: 4194304 || all params: 6742609920 || trainable%: 0.0622\n",
            "10/03/2023 03:32:16 - INFO - llmtuner.extras.template - Add pad token: </s>\n",
            "[INFO|tokenization_utils_base.py:926] 2023-10-03 03:32:16,719 >> Assigning [] to the additional_special_tokens key of the tokenizer\n",
            "Filter:   0% 0/1039 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-07d96443c3ebc601/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-9871e6170b18680f.arrow\n",
            "Filter: 100% 1039/1039 [00:00<00:00, 17893.32 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1039 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-07d96443c3ebc601/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-0830e79a49e0380c.arrow\n",
            "Running tokenizer on dataset: 100% 1039/1039 [00:00<00:00, 1606.04 examples/s]\n",
            "input_ids:\n",
            "[1, 319, 13563, 1546, 263, 12758, 1404, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 1404, 29915, 29879, 5155, 29889, 29871, 13, 12968, 29901, 29871, 30982, 31695, 31863, 31577, 30210, 30457, 30502, 31302, 30858, 30267, 13, 7900, 22137, 29901, 29871, 29871, 30651, 30557, 30392, 30982, 31695, 31863, 31577, 30210, 30457, 30502, 31302, 30858, 30383, 13, 13, 29896, 29889, 29871, 30982, 31695, 31687, 30988, 31704, 30846, 30267, 31951, 30408, 232, 132, 157, 236, 131, 133, 30948, 30210, 31687, 30988, 31894, 30846, 30214, 30847, 233, 152, 166, 233, 176, 168, 30330, 235, 186, 148, 233, 176, 168, 31391, 233, 187, 187, 233, 182, 182, 30214, 30815, 231, 194, 134, 31174, 30869, 235, 164, 131, 31624, 31863, 31577, 30214, 232, 165, 161, 232, 191, 189, 235, 133, 143, 235, 133, 140, 31074, 31180, 30214, 31666, 30417, 31931, 30909, 232, 138, 146, 31022, 30988, 30908, 30267, 13, 13, 29906, 29889, 29871, 232, 160, 138, 235, 164, 164, 236, 168, 177, 31855, 30267, 31951, 30408, 31855, 30406, 30374, 236, 181, 159, 30210, 235, 151, 175, 31854, 30330, 30716, 30801, 30330, 30753, 31112, 30834, 30503, 235, 135, 133, 235, 133, 173, 232, 147, 174, 31180, 231, 192, 145, 30210, 235, 158, 142, 30868, 235, 183, 171, 31855, 30834, 30214, 236, 132, 194, 232, 136, 144, 30528, 234, 182, 153, 30330, 30528, 235, 135, 133, 235, 133, 173, 30503, 30666, 31041, 31855, 31399, 30214, 30651, 30982, 31695, 31863, 31577, 30210, 236, 168, 177, 31855, 231, 188, 163, 233, 134, 178, 30267, 13, 13, 29941, 29889, 29871, 234, 160, 164, 234, 159, 163, 232, 136, 136, 31722, 30267, 234, 160, 164, 234, 159, 163, 30783, 30313, 30988, 31863, 31577, 235, 138, 182, 31057, 30908, 30698, 30214, 30494, 30470, 30313, 31951, 30408, 31370, 30982, 235, 178, 132, 29871, 29955, 29899, 29947, 29871, 30446, 30594, 30210, 234, 160, 164, 234, 159, 163, 30267, 31400, 31076, 30210, 234, 160, 164, 234, 159, 163, 30417, 31931, 30909, 232, 138, 146, 235, 192, 190, 232, 145, 142, 31074, 30214, 231, 194, 134, 31174, 31687, 30988, 233, 132, 165, 31810, 30214, 31666, 31302, 30528, 31368, 31474, 31074, 30503, 31410, 232, 194, 137, 31074, 30267, 2]\n",
            "inputs:\n",
            "<s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. \n",
            " Human: 保持健康的三个提示。\n",
            "Assistant:  以下是保持健康的三个提示：\n",
            "\n",
            "1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\n",
            "\n",
            "2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\n",
            "\n",
            "3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。</s>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 29871, 30651, 30557, 30392, 30982, 31695, 31863, 31577, 30210, 30457, 30502, 31302, 30858, 30383, 13, 13, 29896, 29889, 29871, 30982, 31695, 31687, 30988, 31704, 30846, 30267, 31951, 30408, 232, 132, 157, 236, 131, 133, 30948, 30210, 31687, 30988, 31894, 30846, 30214, 30847, 233, 152, 166, 233, 176, 168, 30330, 235, 186, 148, 233, 176, 168, 31391, 233, 187, 187, 233, 182, 182, 30214, 30815, 231, 194, 134, 31174, 30869, 235, 164, 131, 31624, 31863, 31577, 30214, 232, 165, 161, 232, 191, 189, 235, 133, 143, 235, 133, 140, 31074, 31180, 30214, 31666, 30417, 31931, 30909, 232, 138, 146, 31022, 30988, 30908, 30267, 13, 13, 29906, 29889, 29871, 232, 160, 138, 235, 164, 164, 236, 168, 177, 31855, 30267, 31951, 30408, 31855, 30406, 30374, 236, 181, 159, 30210, 235, 151, 175, 31854, 30330, 30716, 30801, 30330, 30753, 31112, 30834, 30503, 235, 135, 133, 235, 133, 173, 232, 147, 174, 31180, 231, 192, 145, 30210, 235, 158, 142, 30868, 235, 183, 171, 31855, 30834, 30214, 236, 132, 194, 232, 136, 144, 30528, 234, 182, 153, 30330, 30528, 235, 135, 133, 235, 133, 173, 30503, 30666, 31041, 31855, 31399, 30214, 30651, 30982, 31695, 31863, 31577, 30210, 236, 168, 177, 31855, 231, 188, 163, 233, 134, 178, 30267, 13, 13, 29941, 29889, 29871, 234, 160, 164, 234, 159, 163, 232, 136, 136, 31722, 30267, 234, 160, 164, 234, 159, 163, 30783, 30313, 30988, 31863, 31577, 235, 138, 182, 31057, 30908, 30698, 30214, 30494, 30470, 30313, 31951, 30408, 31370, 30982, 235, 178, 132, 29871, 29955, 29899, 29947, 29871, 30446, 30594, 30210, 234, 160, 164, 234, 159, 163, 30267, 31400, 31076, 30210, 234, 160, 164, 234, 159, 163, 30417, 31931, 30909, 232, 138, 146, 235, 192, 190, 232, 145, 142, 31074, 30214, 231, 194, 134, 31174, 31687, 30988, 233, 132, 165, 31810, 30214, 31666, 31302, 30528, 31368, 31474, 31074, 30503, 31410, 232, 194, 137, 31074, 30267, 2]\n",
            "labels:\n",
            "以下是保持健康的三个提示：\n",
            "\n",
            "1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\n",
            "\n",
            "2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\n",
            "\n",
            "3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。</s>\n",
            "[INFO|training_args.py:1332] 2023-10-03 03:32:17,463 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1764] 2023-10-03 03:32:17,463 >> PyTorch: setting up devices\n",
            "[INFO|trainer.py:1715] 2023-10-03 03:32:28,693 >> ***** Running training *****\n",
            "[INFO|trainer.py:1716] 2023-10-03 03:32:28,693 >>   Num examples = 1,039\n",
            "[INFO|trainer.py:1717] 2023-10-03 03:32:28,693 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1718] 2023-10-03 03:32:28,693 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1721] 2023-10-03 03:32:28,693 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1722] 2023-10-03 03:32:28,693 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:1723] 2023-10-03 03:32:28,693 >>   Total optimization steps = 195\n",
            "[INFO|trainer.py:1724] 2023-10-03 03:32:28,696 >>   Number of trainable parameters = 4,194,304\n",
            "  0% 0/195 [00:00<?, ?it/s][WARNING|logging.py:290] 2023-10-03 03:32:28,710 >> You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 1.1978, 'learning_rate': 4.967625656594782e-05, 'epoch': 0.15}\n",
            "  5% 10/195 [00:37<10:41,  3.47s/it][INFO|trainer.py:2855] 2023-10-03 03:33:06,534 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-10\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:33:06,570 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-10/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:33:06,571 >> Special tokens file saved in largitdata_ckpoint/checkpoint-10/special_tokens_map.json\n",
            "{'loss': 1.1601, 'learning_rate': 4.8713411048678635e-05, 'epoch': 0.31}\n",
            " 10% 20/195 [01:08<09:07,  3.13s/it][INFO|trainer.py:2855] 2023-10-03 03:33:37,331 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-20\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:33:37,366 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-20/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:33:37,366 >> Special tokens file saved in largitdata_ckpoint/checkpoint-20/special_tokens_map.json\n",
            "{'loss': 1.0943, 'learning_rate': 4.713640064133025e-05, 'epoch': 0.46}\n",
            " 15% 30/195 [01:38<08:48,  3.20s/it][INFO|trainer.py:2855] 2023-10-03 03:34:07,533 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-30\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:34:07,569 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-30/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:34:07,569 >> Special tokens file saved in largitdata_ckpoint/checkpoint-30/special_tokens_map.json\n",
            "{'loss': 1.0375, 'learning_rate': 4.498606908508754e-05, 'epoch': 0.62}\n",
            " 21% 40/195 [02:08<07:26,  2.88s/it][INFO|trainer.py:2855] 2023-10-03 03:34:37,163 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-40\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:34:37,198 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-40/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:34:37,198 >> Special tokens file saved in largitdata_ckpoint/checkpoint-40/special_tokens_map.json\n",
            "{'loss': 1.0614, 'learning_rate': 4.231810883773999e-05, 'epoch': 0.77}\n",
            " 26% 50/195 [02:40<07:38,  3.16s/it][INFO|trainer.py:2855] 2023-10-03 03:35:08,948 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-50\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:35:08,984 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-50/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:35:08,984 >> Special tokens file saved in largitdata_ckpoint/checkpoint-50/special_tokens_map.json\n",
            "{'loss': 1.0335, 'learning_rate': 3.920161866827889e-05, 'epoch': 0.92}\n",
            " 31% 60/195 [03:12<07:47,  3.46s/it][INFO|trainer.py:2855] 2023-10-03 03:35:41,311 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-60\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:35:41,347 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-60/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:35:41,347 >> Special tokens file saved in largitdata_ckpoint/checkpoint-60/special_tokens_map.json\n",
            "{'loss': 0.9466, 'learning_rate': 3.5717314035076355e-05, 'epoch': 1.08}\n",
            " 36% 70/195 [03:44<06:25,  3.09s/it][INFO|trainer.py:2855] 2023-10-03 03:36:13,029 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-70\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:36:13,065 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-70/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:36:13,065 >> Special tokens file saved in largitdata_ckpoint/checkpoint-70/special_tokens_map.json\n",
            "{'loss': 0.9499, 'learning_rate': 3.195543659791132e-05, 'epoch': 1.23}\n",
            " 41% 80/195 [04:16<05:50,  3.05s/it][INFO|trainer.py:2855] 2023-10-03 03:36:45,174 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-80\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:36:45,209 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-80/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:36:45,209 >> Special tokens file saved in largitdata_ckpoint/checkpoint-80/special_tokens_map.json\n",
            "{'loss': 0.9744, 'learning_rate': 2.8013417006383076e-05, 'epoch': 1.38}\n",
            " 46% 90/195 [04:49<05:29,  3.14s/it][INFO|trainer.py:2855] 2023-10-03 03:37:18,392 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-90\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:37:18,430 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-90/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:37:18,430 >> Special tokens file saved in largitdata_ckpoint/checkpoint-90/special_tokens_map.json\n",
            "{'loss': 0.9917, 'learning_rate': 2.399335149726463e-05, 'epoch': 1.54}\n",
            " 51% 100/195 [05:21<04:56,  3.13s/it][INFO|trainer.py:2855] 2023-10-03 03:37:50,378 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-100\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:37:50,414 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:37:50,414 >> Special tokens file saved in largitdata_ckpoint/checkpoint-100/special_tokens_map.json\n",
            "{'loss': 0.9915, 'learning_rate': 1.9999357655598893e-05, 'epoch': 1.69}\n",
            " 56% 110/195 [05:50<03:53,  2.75s/it][INFO|trainer.py:2855] 2023-10-03 03:38:19,522 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-110\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:38:19,558 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-110/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:38:19,558 >> Special tokens file saved in largitdata_ckpoint/checkpoint-110/special_tokens_map.json\n",
            "{'loss': 1.0012, 'learning_rate': 1.613487782393661e-05, 'epoch': 1.85}\n",
            " 62% 120/195 [06:22<03:57,  3.17s/it][INFO|trainer.py:2855] 2023-10-03 03:38:51,508 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-120\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:38:51,544 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-120/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:38:51,544 >> Special tokens file saved in largitdata_ckpoint/checkpoint-120/special_tokens_map.json\n",
            "{'loss': 0.9248, 'learning_rate': 1.2500000000000006e-05, 'epoch': 2.0}\n",
            " 67% 130/195 [06:55<03:27,  3.20s/it][INFO|trainer.py:2855] 2023-10-03 03:39:24,369 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-130\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:39:24,404 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-130/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:39:24,405 >> Special tokens file saved in largitdata_ckpoint/checkpoint-130/special_tokens_map.json\n",
            "{'loss': 0.9257, 'learning_rate': 9.18886561011557e-06, 'epoch': 2.15}\n",
            " 72% 140/195 [07:29<03:04,  3.35s/it][INFO|trainer.py:2855] 2023-10-03 03:39:58,329 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-140\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:39:58,365 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-140/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:39:58,366 >> Special tokens file saved in largitdata_ckpoint/checkpoint-140/special_tokens_map.json\n",
            "{'loss': 0.962, 'learning_rate': 6.28723129572247e-06, 'epoch': 2.31}\n",
            " 77% 150/195 [07:59<02:16,  3.03s/it][INFO|trainer.py:2855] 2023-10-03 03:40:28,435 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-150\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:40:28,470 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-150/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:40:28,470 >> Special tokens file saved in largitdata_ckpoint/checkpoint-150/special_tokens_map.json\n",
            "{'loss': 0.9525, 'learning_rate': 3.8702478614051355e-06, 'epoch': 2.46}\n",
            " 82% 160/195 [08:33<01:54,  3.26s/it][INFO|trainer.py:2855] 2023-10-03 03:41:02,031 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-160\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:41:02,067 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-160/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:41:02,067 >> Special tokens file saved in largitdata_ckpoint/checkpoint-160/special_tokens_map.json\n",
            "{'loss': 0.9655, 'learning_rate': 2.0005139085293945e-06, 'epoch': 2.62}\n",
            " 87% 170/195 [09:03<01:23,  3.34s/it][INFO|trainer.py:2855] 2023-10-03 03:41:31,782 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-170\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:41:31,818 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-170/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:41:31,818 >> Special tokens file saved in largitdata_ckpoint/checkpoint-170/special_tokens_map.json\n",
            "{'loss': 0.9676, 'learning_rate': 7.264545643486997e-07, 'epoch': 2.77}\n",
            " 92% 180/195 [09:37<00:47,  3.17s/it][INFO|trainer.py:2855] 2023-10-03 03:42:05,829 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-180\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:42:05,865 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-180/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:42:05,866 >> Special tokens file saved in largitdata_ckpoint/checkpoint-180/special_tokens_map.json\n",
            "{'loss': 0.9243, 'learning_rate': 8.106729664475176e-08, 'epoch': 2.92}\n",
            " 97% 190/195 [10:08<00:15,  3.08s/it][INFO|trainer.py:2855] 2023-10-03 03:42:36,789 >> Saving model checkpoint to largitdata_ckpoint/checkpoint-190\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:42:36,824 >> tokenizer config file saved in largitdata_ckpoint/checkpoint-190/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:42:36,824 >> Special tokens file saved in largitdata_ckpoint/checkpoint-190/special_tokens_map.json\n",
            "100% 195/195 [10:22<00:00,  2.91s/it][INFO|trainer.py:1963] 2023-10-03 03:42:51,385 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 622.6899, 'train_samples_per_second': 5.006, 'train_steps_per_second': 0.313, 'train_loss': 1.0021348488636506, 'epoch': 3.0}\n",
            "100% 195/195 [10:22<00:00,  3.19s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     1.0021\n",
            "  train_runtime            = 0:10:22.68\n",
            "  train_samples_per_second =      5.006\n",
            "  train_steps_per_second   =      0.313\n",
            "[INFO|trainer.py:2855] 2023-10-03 03:42:51,389 >> Saving model checkpoint to largitdata_ckpoint\n",
            "[INFO|tokenization_utils_base.py:2235] 2023-10-03 03:42:51,424 >> tokenizer config file saved in largitdata_ckpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2242] 2023-10-03 03:42:51,424 >> Special tokens file saved in largitdata_ckpoint/special_tokens_map.json\n",
            "Figure saved: largitdata_ckpoint/training_loss.png\n",
            "10/03/2023 03:42:51 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git config --global user.email \"dpm4212@outlook.com\""
      ],
      "metadata": {
        "id": "syRdFcBm-Uq6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB2X8bLYqQdR",
        "outputId": "93ff7bee-c4d6-48df-c5b7-395e8cf32ac7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reinitialized existing Git repository in /content/LLaMA-Efficient-Tuning/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git add README.md"
      ],
      "metadata": {
        "id": "nvz9GiijSN44"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git commit -m \"first commit. add dataset largitdata\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBCA000cpQwY",
        "outputId": "7e93984e-f014-4437-c802-34b792c247b7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is ahead of 'origin/main' by 1 commit.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git branch -M main"
      ],
      "metadata": {
        "id": "LtWju8z-pUhZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git remote add origin git@github.com:ZeraPeng/llama-finetuning.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMkWoDkMRuSa",
        "outputId": "6cdc5ec2-b7ff-46a4-ba33-4ee2337e4c7a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: remote origin already exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git push -u origin main -f"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAMS4BlESdjQ",
        "outputId": "001a9e78-bfff-4ff0-be56-031177e09890"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git add ."
      ],
      "metadata": {
        "id": "MgGYeyg5S-D2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! git commit -m \"first commit\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r0yvMGlTBwc",
        "outputId": "0b956aad-c1ed-4000-c08e-82e490ad01b3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is ahead of 'origin/main' by 1 commit.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    }
  ]
}